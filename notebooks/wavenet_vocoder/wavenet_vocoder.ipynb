{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FubxMlZ7R1UQ"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/wavenet_vocoder.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ0C_5O8R1UR"
      },
      "source": [
        "# WaveNet Vocoder Recipe Demonstration\n",
        "\n",
        "**Tomoki Hayashi**\n",
        "\n",
        "Department of Informatics, Nagoya University  \n",
        "Human Dataware Lab. Co., Ltd."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHnmcEeuR1UT"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40eYA0VBR1UU"
      },
      "source": [
        "## Environmental setup\n",
        "\n",
        "First, install dependecies (It takes several minutes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "2"
        },
        "scrolled": true,
        "id": "bFDlLJe5R1UU"
      },
      "outputs": [],
      "source": [
        "!apt-get install -qq -y bc tree\n",
        "!git clone https://github.com/kan-bayashi/PytorchWaveNetVocoder.git -b IS19TUTORIAL\n",
        "!git clone https://github.com/k2kobayashi/sprocket.git -b IS19TUTORIAL\n",
        "!cd sprocket && pip install -q .\n",
        "!cd PytorchWaveNetVocoder && pip install -q .\n",
        "!cd PytorchWaveNetVocoder && mkdir -p tools/venv/bin && touch tools/venv/bin/activate\n",
        "import sprocket, wavenet_vocoder  # check importable\n",
        "!echo \"Setup done!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8sg9MUOR1UU"
      },
      "source": [
        "## What is the PytorchWaveNetVocoder?\n",
        "\n",
        "Github: [kan-bayashi/PytorchWaveNetVocoder](https://github.com/kan-bayashi/PytorchWaveNetVocoder)  \n",
        "Samples: https://kan-bayashi.github.io/WaveNetVocoderSamples/\n",
        "\n",
        "- WaveNet vocoder implemention with pytorch\n",
        "- Support [kaldi](https://github.com/kaldi-asr/kaldi)-like recipes, easy to reproduce the results\n",
        "- Support [World](https://github.com/mmorise/World) features / mel-spectrogram based models\n",
        "- Support multi-gpu training / decoding\n",
        "- Support a noise shaping [[Tachibana+ 2018](https://ieeexplore.ieee.org/document/8461332)]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X02mPDzfR1UV"
      },
      "source": [
        "## What it the kaldi-like recipe?\n",
        "\n",
        "Key features:\n",
        "- Prepared for each corpus (e.g. CMU Arctic, LJSpeech)\n",
        "- Consists of unified several stages  \n",
        "  (e.g. data preparation, feature extraction, and so on.)\n",
        "- Includes all procedures needed to reproduce the results\n",
        "- All of the recipes are stored in `egs/<corpus>/<type>`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWvxlQp7R1UW"
      },
      "source": [
        "Supported corpus:\n",
        "- [CMUArctic database](http://www.festvox.org/cmu_arctic/): `egs/arctic`, 16 kHz, English, Several speakers.\n",
        "- [LJ Speech database](https://keithito.com/LJ-Speech-Dataset/): `egs/ljspeech` 22.05 kHz, English, Single female speaker.\n",
        "- [M-AILABS speech database](http://www.m-ailabs.bayern/en/the-mailabs-speech-dataset/):`egs/m-ailabs-speech`: 16 kHz, various speakers\n",
        "\n",
        "About supported type, see detail in https://github.com/kan-bayashi/PytorchWaveNetVocoder/tree/master/egs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zoZ__C6R1UW"
      },
      "source": [
        "## Run the demo recipe\n",
        "\n",
        "Let us run the demo recipe `egs/arctic/sd-mini`.\n",
        "\n",
        "- Small version of `egs/arctic/sd`\n",
        "- Use subset of all of the utterances\n",
        "- **Cannot build a good model** but the flow is **the same**\n",
        "\n",
        "You can understand each stage within 30 minutes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "8"
        },
        "id": "N6_wQy23R1UX"
      },
      "outputs": [],
      "source": [
        "# move on the recipe directory\n",
        "import os\n",
        "os.chdir(\"./PytorchWaveNetVocoder/egs/arctic/sd-mini\")\n",
        "!echo $(pwd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNlG3WqzR1UX"
      },
      "source": [
        "Files in the recipe are as follows:\n",
        "- `conf`: Directory including config files\n",
        "- `path.sh`: Script to set the environmental variables.\n",
        "- `run.sh`: Main script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "9"
        },
        "id": "tTYgIUZ-R1UX"
      },
      "outputs": [],
      "source": [
        "!tree -L 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHPq6blmR1UX"
      },
      "source": [
        "`conf` includes f0 setting files whose name format is `<speaker_name>.f0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "13"
        },
        "id": "hchKuSBCR1UY"
      },
      "outputs": [],
      "source": [
        "!ls conf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-zWC_VmR1UY"
      },
      "source": [
        "`<speaker_name>.f0` includes `min_f0 max_f0`.  \n",
        "These values are predecided by ourselve, so you can modify them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFLJV_EdR1UY"
      },
      "outputs": [],
      "source": [
        "!cat conf/slt.f0  # (minf0 maxf0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr-d87RoR1UY"
      },
      "source": [
        "All of the hyperparameters are written in `run.sh`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "uKyWFhgIR1UY"
      },
      "outputs": [],
      "source": [
        "!head -n 69 run.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNZQfhstR1UY"
      },
      "source": [
        "Let us introduce these parameters in detail later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "4"
        },
        "id": "mljmNqQZR1UZ"
      },
      "outputs": [],
      "source": [
        "# (Optional) here you can add your command to check the file!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCYEtEHqR1UZ"
      },
      "source": [
        "### Overview of the recipe\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/overview.png?raw=1 width=80%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S0HEVvHR1UZ"
      },
      "source": [
        "If run `run.sh`, all of stages will be performed.\n",
        "\n",
        "But we can specify the stage to run with `--stage` options.\n",
        "\n",
        "- `run.sh --stage 0`: Run only the stage 0\n",
        "- `run.sh --stage 012`: Run the stages 0, 1, and 2.\n",
        "\n",
        "Here, let us run each stage step-by-step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYQD6E2VR1UZ"
      },
      "source": [
        "### Stage 0: Data preparation\n",
        "\n",
        "This stage performs download of corpus and list preparation.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/stage_0.png?raw=1 width=70%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUq4_K24R1UZ"
      },
      "source": [
        "In arctic, there are seven speakers.  \n",
        "Here let us use `slt` to build a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "11"
        },
        "scrolled": false,
        "id": "juYgLB49R1UZ"
      },
      "outputs": [],
      "source": [
        "# you can specify the speaker via --spk (default=slt)\n",
        "!./run.sh --stage 0 --spk slt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqYpOJJGR1UZ"
      },
      "source": [
        "Corpus is saved in\n",
        "- `downloads/cmu_us_<spk_name>_arctic_mini`\n",
        "\n",
        "Two lists of wav files are created.\n",
        "- `data/tr_slt/wav.scp`: wav list file for training\n",
        "- `data/ev_slt/wav.scp`: wav list file for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "12"
        },
        "id": "-NQqV3D8R1Ua"
      },
      "outputs": [],
      "source": [
        "!tree -L 3 -I local"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zODOcaY5R1Ua"
      },
      "source": [
        "The list file is that:\n",
        "- Each line has the path of wav file\n",
        "- All of the lines are sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "13"
        },
        "id": "bFdn9VikR1Ua"
      },
      "outputs": [],
      "source": [
        " !head -n 3 data/*_slt/wav.scp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq3mCOR5R1Ua"
      },
      "source": [
        "Here we use 32 utts for training, 4 for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DMgOO9kR1Ua"
      },
      "outputs": [],
      "source": [
        "!wc -l < data/tr_slt/wav.scp\n",
        "!wc -l < data/ev_slt/wav.scp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3vPK3NKR1Ua"
      },
      "outputs": [],
      "source": [
        "# (Optional) here you can check the file with your commands!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Neu125bR1Ub"
      },
      "source": [
        "### Stage 1: Feature extraction\n",
        "\n",
        "This stage performs feature extraction with the\n",
        "list file.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/stage_1.png?raw=1 width=70%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "8Uuu6nDaR1Ub"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters related to stage 1\n",
        "!head -n 36 run.sh | tail -n 13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "14"
        },
        "scrolled": false,
        "id": "f5JG06GHR1Uc"
      },
      "outputs": [],
      "source": [
        "# run stage 1 with default settings\n",
        "!./run.sh --stage 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpyU0mtpR1Uc"
      },
      "source": [
        "Hyperparameters can be changed via command line.  \n",
        "But it will overwrite the existing ones. Be careful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "NdFS2brXR1Uc"
      },
      "outputs": [],
      "source": [
        "# example of changing hyperparameters of feature extraction\n",
        "# !./run.sh --stage 1 --mcep_dim 30 --shiftms 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNbC5xHyR1Ue"
      },
      "source": [
        "Extracted features are saved as `hdf5` in\n",
        "- `hdf5/tr_slt/*.h5`: Feature file of training data\n",
        "- `hdf5/ev_slt/*.h5`: Feature file of evaluation data\n",
        "\n",
        "Lists of feature files are created\n",
        "- `data/tr_slt/feats.scp`\n",
        "- `data/ev_slt/feats.scp`\n",
        "\n",
        "High pass filtered training wav files are saved in\n",
        "- `wav_hpf/tr_slt/*.wav`: Filtered wav file of training data\n",
        "\n",
        "List of filetered wav files is created\n",
        "- `data/tr_slt/wav_hpf.scp`: List of filtered wav files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "15"
        },
        "scrolled": false,
        "id": "MgKi-6v8R1Ug"
      },
      "outputs": [],
      "source": [
        "!tree -L 3 -I \"*.f0|local|cmu_*\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OAE1VE9R1Uk"
      },
      "source": [
        "Let us check the list file format:\n",
        "- Each line has the path of feature or wav\n",
        "file  \n",
        "- All of the lines are sorted\n",
        "- Assume that all of the lists has the same\n",
        "order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "16"
        },
        "id": "VrMtGd4aR1Uk"
      },
      "outputs": [],
      "source": [
        "!head -n 3 data/*_slt/feats.scp\n",
        "!echo \"\"\n",
        "!head -n 3 data/tr_slt/wav_hpf.scp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8V7Iq_cR1Uk"
      },
      "source": [
        "hdf5 format can be loaded as `numpy.ndarray` in python using `h5py` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "17"
        },
        "id": "HqsEZ7FmR1Uk"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "with h5py.File(\"hdf5/tr_slt/arctic_a0001.h5\") as f:\n",
        "    print(f.keys())\n",
        "    feat = f[\"world\"][()]\n",
        "# or you can use our utils\n",
        "from wavenet_vocoder.utils import read_hdf5\n",
        "feat = read_hdf5(\"hdf5/tr_slt/arctic_a0001.h5\", \"world\")\n",
        "print(\"Feature shape: (#num_frames=%d, #feature_dims=%d)\" % (feat.shape[0], feat.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymkTh7RNR1Ul"
      },
      "source": [
        "The feature is extracted with World.\n",
        "- `U/V binary` (1 dim)\n",
        "- `continuous F0` (1 dim),\n",
        "- `mcep`(25 dim = `mcep_dim + 1`)\n",
        "- `ap` (1 dim)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "18"
        },
        "id": "mcdWFCTgR1Ul"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(16, 9))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(feat[:, 0])\n",
        "plt.title(\"U/V binary\")\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(feat[:, 1])\n",
        "plt.title(\"Continuous F0\")\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.imshow(feat[:, 2:26].T, aspect=\"auto\")\n",
        "plt.title(\"Mel-cepstrum\")\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(feat[:, -1])\n",
        "plt.title(\"Aperiodicity\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgso_qhIR1Ul"
      },
      "outputs": [],
      "source": [
        "# (Optional) here you can check the file with your commands!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol3RV3wFR1Ul"
      },
      "source": [
        "### Stage 2: Statistics calculation\n",
        "\n",
        "This stage calculates the mean and variance of features.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/stage_2.png?raw=1 width=70%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "19"
        },
        "scrolled": false,
        "id": "iPOYuaDlR1Ul"
      },
      "outputs": [],
      "source": [
        "# run stage 2 with default settings\n",
        "!./run.sh --stage 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_esC_PeR1Ul"
      },
      "source": [
        "Calculated statistics are saved as `hdf5` format in\n",
        "- `data/tr_slt/stats.h5`\n",
        "\n",
        "`stats.h5` is used for:\n",
        "- Feature normalization during training\n",
        "- Calculation of noise shaping filter coefficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "20"
        },
        "id": "cf9zfZqbR1Ul"
      },
      "outputs": [],
      "source": [
        "!tree -L 3 -I \"*.f0|*.wav|*[0-9].h5|local|cmu_*\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ60ACA8R1Ul"
      },
      "source": [
        "`stats.h5` can be loaded as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "21"
        },
        "id": "OvSADPlHR1Um"
      },
      "outputs": [],
      "source": [
        "with h5py.File(\"data/tr_slt/stats.h5\") as f:\n",
        "    print(f.keys())\n",
        "    print(f['world'].keys())\n",
        "    mean = f['world']['mean'][()]\n",
        "    scale = f['world']['scale'][()]\n",
        "    print(mean.shape)\n",
        "    print(scale.shape)\n",
        "\n",
        "# or you use our utils\n",
        "mean = read_hdf5(\"data/tr_slt/stats.h5\", \"world/mean\")\n",
        "scale = read_hdf5(\"data/tr_slt/stats.h5\", \"world/scale\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5tUj-uVR1Um"
      },
      "outputs": [],
      "source": [
        "# here you can check the file with your commands!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVUhZS1mR1Um"
      },
      "source": [
        "### Stage 3: Noise weighting\n",
        "\n",
        "This stage applies noise weighting filter to training\n",
        "wav files.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/stage_3.png?raw=1 width=70%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKINnTRuR1Um"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters related to stage 3\n",
        "!head -n 38 run.sh | tail -n 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "22"
        },
        "id": "N8x5Mhd7R1Un"
      },
      "outputs": [],
      "source": [
        "# run stage 3 with default settings\n",
        "!./run.sh --stage 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxlz4o4NR1Un"
      },
      "source": [
        "If `use_noise_shaping=false`, `stage 3` will be skipped."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kk620ysR1Un"
      },
      "source": [
        "Noise weighting filtered wav files are saved in\n",
        "- `wav_nwf/tr_slt/*.wav`\n",
        "\n",
        "The list of noise weighting filtered wav files is saved as\n",
        "- `data/tr_slt/wav_nwf.scp`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "23"
        },
        "id": "LwybmOBoR1Un"
      },
      "outputs": [],
      "source": [
        "!tree -L 3 -I \"*.f0|*[0-9].h5|local|cmu_*\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GG8SjPCR1Un"
      },
      "source": [
        "Let us check the difference of waveform here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "25"
        },
        "id": "Muf0Kom2R1Uo"
      },
      "outputs": [],
      "source": [
        "# listen to the samples\n",
        "import IPython.display\n",
        "IPython.display.display(IPython.display.Audio(\"wav_hpf/tr_slt/arctic_a0001.wav\"))\n",
        "IPython.display.display(IPython.display.Audio(\"wav_nwf/tr_slt/arctic_a0001.wav\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "26"
        },
        "id": "q8zVTPtxR1Uo"
      },
      "outputs": [],
      "source": [
        "# show spectrogram\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "x, fs = sf.read(\"wav_hpf/tr_slt/arctic_a0001.wav\")\n",
        "x_ns, fs = sf.read(\"wav_nwf/tr_slt/arctic_a0001.wav\")\n",
        "plt.figure(figsize=(16, 7))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.specgram(x, Fs=fs)\n",
        "plt.title(\"Original spectrogram\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.specgram(x_ns, Fs=fs)\n",
        "plt.title(\"Noise weighting filtered spectrogram\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86FLwOHOR1Uo"
      },
      "source": [
        "Filtering related parameters `mlas/coef` and `mlsa/alpha` are added in `data/tr_slt/stats.h5`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "27"
        },
        "id": "Hf_gHrr_R1Uo"
      },
      "outputs": [],
      "source": [
        "with h5py.File(\"data/tr_slt/stats.h5\") as f:\n",
        "    print(f.keys())\n",
        "    print(f[\"mlsa\"].keys())\n",
        "    print(f[\"mlsa\"][\"alpha\"])\n",
        "    print(f[\"mlsa\"][\"coef\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS1LzNYXR1Up"
      },
      "source": [
        "`mlsa/coef` is the coefficient of MLSA filter, which is calculated from averaged mel-cepstrum and `mag`.  \n",
        "`mlsa/alpha` is the hyperparameter `alpha`, all pass filter coefficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CruXPuXJR1Up"
      },
      "outputs": [],
      "source": [
        "# (Optional) here you can check the file with your commands!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19laDAkhR1Up"
      },
      "source": [
        "### Stage 4: WaveNet training\n",
        "\n",
        "This stage trains WaveNet using extracted\n",
        "features and noise weighting filtered wav files.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/stage_4.png?raw=1 width=70%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpJnUNsTR1Up"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters related to stage 4\n",
        "!head -n 59 run.sh | tail -n 19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "28"
        },
        "scrolled": false,
        "id": "-0NzxKwbR1Uq"
      },
      "outputs": [],
      "source": [
        "# run stage 4 with default settings\n",
        "!./run.sh --stage 4 --iters 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfAUGizxR1Uq"
      },
      "source": [
        "Default network structure in `egs/arctic/sd-mini`.\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/wavenet.png?raw=1 width=70%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HPjpYnVR1Uq"
      },
      "source": [
        "Example when `dilation_depth=3` and `dilation_repeat=2`.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/structure_ex.png?raw=1 width=45%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB8ho7lKR1Uq"
      },
      "source": [
        "Make a batch by split a waveform into pieces.\n",
        "- `batch_size`: Number of batches\n",
        "- `batch_length`: Length of each batch\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/batch.png?raw=1 width=65%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC5ow7y1R1Uq"
      },
      "source": [
        "Model parameters are saved as  \n",
        "- `exp/tr_arctic_16k_sd_world_slt_*/checkpoint-*.pkl`\n",
        "\n",
        "Modle configuration is saved as  \n",
        "- `exp/tr_arctic_16k_sd_world_slt_*/model.conf`\n",
        "\n",
        "The directory name is automatically set to be unique depending on hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "29"
        },
        "id": "o6lW9G2rR1Uq"
      },
      "outputs": [],
      "source": [
        "!tree -L 3 -I \"*.f0|*.wav|*[0-9].h5|*.scp|*.log|local|cmu_*\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN-KD9WcR1Ur"
      },
      "source": [
        "Model configuration file can be loaded as `argparse.Namespace`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "30"
        },
        "id": "HZSD5-9vR1Ur"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "conf = torch.load(\"exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/model.conf\")\n",
        "print(conf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgACV8J5R1Ur"
      },
      "source": [
        "Model parameters `checkpoint-*.pkl` can be loaded as `dict` which contains\n",
        "following information:\n",
        "- `iterations`: Number of iterations of this parameters\n",
        "- `optimizer`: `Dict` of states of optimizer\n",
        "- `model`: `OrderedDict` of Model\n",
        "parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "31"
        },
        "id": "ixigQlxlR1Ut"
      },
      "outputs": [],
      "source": [
        "state_dict = torch.load(\"exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/checkpoint-500.pkl\")\n",
        "print(state_dict.keys())\n",
        "print(state_dict[\"iterations\"])\n",
        "print(state_dict[\"optimizer\"].keys())\n",
        "print(state_dict[\"model\"].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzhk8buKR1Uu"
      },
      "source": [
        "You can resume training from `checkpoint-*.pkl` file with `--resume` options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "32"
        },
        "scrolled": false,
        "id": "vLWM8fyLR1Uu"
      },
      "outputs": [],
      "source": [
        "!./run.sh --stage 4 \\\n",
        "    --iters 1000 \\\n",
        "    --resume exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/checkpoint-500.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJnjJz33R1Uu"
      },
      "source": [
        "You can train using multi-gpu with `--n_gpus` option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlCZ5p7yR1Uu"
      },
      "outputs": [],
      "source": [
        "# In colab, we can use only a single gpu :(\n",
        "# batch_size must be >= n_gpus\n",
        "# !./run.sh --stage 4 --n_gpus 2 --batch_size 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGGaGd-PR1Uv"
      },
      "outputs": [],
      "source": [
        "# here you can check the file with your commands!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Soa0a6yR1Uv"
      },
      "source": [
        "### Stage 5: WaveNet decoding\n",
        "\n",
        "This stage performs decoding of evaluation data.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/stage_5.png?raw=1 width=70%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxXwQsVIR1Uv"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters related to stage 5\n",
        "!head -n 69 run.sh | tail -n 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "35"
        },
        "id": "JlnAP3O4R1Uv"
      },
      "outputs": [],
      "source": [
        "# run stage 5 with default setting\n",
        "!./run.sh --stage 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzWyMgu-R1Uw"
      },
      "source": [
        "You can specify the `checkpoint-*.pkl` file used for decoding and directory to\n",
        "be saved via `--checkpoint` and `--outdir` options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyFsVD42R1Uw"
      },
      "outputs": [],
      "source": [
        "# it takes times, comment out\n",
        "# !./run.sh --stage 5 \\\n",
        "#     --checkpoint exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/checkpoint-100.pkl\n",
        "#     --outdir exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/wav_ckpt_100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLqfht6YR1Uw"
      },
      "source": [
        "We can use multi-gpu decoding via `--n_gpus` option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiPmfMoTR1Ux"
      },
      "outputs": [],
      "source": [
        "# In colab, we can use only a single gpu :(\n",
        "# !./run.sh --stage 5 --n_gpus 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj-1QFpDR1Ux"
      },
      "source": [
        "Generated wav files are saved in\n",
        "- `exp/tr_arctic_sd_tr_arctic_16k_sd_*/wav`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "36"
        },
        "id": "2BZvTCPmR1Ux"
      },
      "outputs": [],
      "source": [
        "!tree exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHwEJi7cR1Ux"
      },
      "outputs": [],
      "source": [
        "# (Optional) here you can check the file with your commands!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwlC-R7fR1Uy"
      },
      "source": [
        "### Stage 6: Noise shaping\n",
        "\n",
        "This stage applies noise shaping filter to generated wav files.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=https://github.com/kan-bayashi/INTERSPEECH19_TUTORIAL/blob/master/notebooks/wavenet_vocoder/figs/stage_6.png?raw=1 width=70%>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "37"
        },
        "id": "N3BI518RR1Uy"
      },
      "outputs": [],
      "source": [
        "# run stage 6 with default setting\n",
        "!./run.sh --stage 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFssYRRiR1Uy"
      },
      "source": [
        "Restored wav files are saved in\n",
        "\n",
        "- `exp/tr_arctic_sd_tr_arctic_16k_sd_*/wav_nsf`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "38"
        },
        "scrolled": false,
        "id": "_KA0-PH3R1Uy"
      },
      "outputs": [],
      "source": [
        "!tree exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do_n_koWR1Uy"
      },
      "outputs": [],
      "source": [
        "# (Optional) here you can check the file with your commands!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j7SHY5hR1Uy"
      },
      "source": [
        "Finished! Unfortunately, generated samples are just-like a noise.  \n",
        "So Let us check the samples which trained with `egs/arctic/sd` from  \n",
        "https://kan-bayashi.github.io/WaveNetVocoderSamples/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB3lIIg9R1Uy"
      },
      "source": [
        "## Use pretrained model as vocoder\n",
        "\n",
        "Here we show how-to-use pretrained model as\n",
        "vocoder.  \n",
        "What we need to prepare is following three files:\n",
        "\n",
        "- `model.conf`:\n",
        "Model configuration file.\n",
        "- `checkpoint-*.pkl`: Model parameter file.\n",
        "- `stats.h5`: Statistics file.\n",
        "\n",
        "Let us pack following files into\n",
        "`pretrained_model/` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "39"
        },
        "id": "3B3t8XheR1Uy"
      },
      "outputs": [],
      "source": [
        "# summarize trained model in the directory\n",
        "!mkdir pretrained_model\n",
        "!cp -v exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/stats.h5 \\\n",
        "    exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/model.conf \\\n",
        "    exp/tr_arctic_16k_sd_world_slt_nq256_na28_nrc32_nsc16_ks2_dp5_dr1_lr1e-4_wd0.0_bl10000_bs1_ns_up/checkpoint-1000.pkl \\\n",
        "    pretrained_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beTkr6vsR1Uz"
      },
      "source": [
        "First, please prepare the list file of feature files to be decoded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "40"
        },
        "id": "mfuWtKKrR1Uz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# here make a dummy features and the stored as hdf5 with key \"/world\"\n",
        "os.makedirs(\"dummy\", exist_ok=True)\n",
        "for idx, n_frames in enumerate([10, 20, 30, 40]):\n",
        "    x = np.random.randn(n_frames, 28)  # (#num_frames, #feature_dims)\n",
        "    with h5py.File(\"dummy/dummy_%d.h5\" % idx, \"w\") as f:\n",
        "        f[\"world\"] = x\n",
        "\n",
        "# make a list of features to be decoded.\n",
        "!find dummy -name \"*.h5\" > dummy_feats.scp\n",
        "\n",
        "# check\n",
        "!cat dummy_feats.scp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpP3cTh2R1Uz"
      },
      "source": [
        "Run the `--stage 56` by specifying `--feats` in the recipe directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "41"
        },
        "id": "CqDxXTfqR1Uz"
      },
      "outputs": [],
      "source": [
        "# decode with pretrained model through the recipe\n",
        "!./run.sh --stage 56 \\\n",
        "    --outdir dummy_feats_wav \\\n",
        "    --feats dummy_feats.scp \\\n",
        "    --checkpoint pretrained_model/checkpoint-1000.pkl\n",
        "!ls dummy_feats_wav*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu4uY24TR1U0"
      },
      "source": [
        "If you want to use outside of the recipe, directly call python scripts stored in\n",
        "`wavenet_vocoder/bin`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "42"
        },
        "id": "eT45IS9aR1U0"
      },
      "outputs": [],
      "source": [
        "# decode with pretrained model\n",
        "!python ../../../wavenet_vocoder/bin/decode.py \\\n",
        "     --feats dummy_feats.scp \\\n",
        "     --outdir dummy_feats_wav_2 \\\n",
        "     --checkpoint pretrained_model/checkpoint-1000.pkl \\\n",
        "     --fs 16000 \\\n",
        "     --n_gpus 1 \\\n",
        "     --batch_size 4\n",
        "# make list of wav files to be filtered\n",
        "!find dummy_feats_wav_2 -name \"*.wav\" > dummy_feats_wav_2/wav.scp\n",
        "# apply noise shaping filter\n",
        "!python ../../../wavenet_vocoder/bin/noise_shaping.py \\\n",
        "     --waveforms dummy_feats_wav_2/wav.scp \\\n",
        "     --outdir dummy_feats_wav_2_nsf \\\n",
        "     --stats pretrained_model/stats.h5 \\\n",
        "     --fs 16000 \\\n",
        "     --shiftms 5\n",
        "!ls dummy_feats_wav_2*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9crnLTDR1U0"
      },
      "source": [
        "## Combine with Sprocket\n",
        "\n",
        "Let us show how-to-combine wavenet vocoder with voice conversion toolkit [sprocket](https://github.com/k2kobayashi/sprocket).    \n",
        "Here, we generate converted voice with pretrained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_wrIgyNR1U0"
      },
      "outputs": [],
      "source": [
        "# changed directory\n",
        "!mkdir ../../../../conversion_example\n",
        "os.chdir(\"../../../../conversion_example\")\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cY8K6yGR1U0"
      },
      "source": [
        "First, download pretrained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-e8d4XfbR1U0"
      },
      "outputs": [],
      "source": [
        "# download sprocket model\n",
        "!../PytorchWaveNetVocoder/wavenet_vocoder/utils/download_from_google_drive.sh \\\n",
        "    \"https://drive.google.com/open?id=1PiGDyYDQt0b4h6KAV1MOmDxHjHUv1cT6\" \\\n",
        "    downloads/sprocket_pretrained\n",
        "\n",
        "# download wavenet vocoder model\n",
        "!../PytorchWaveNetVocoder/wavenet_vocoder/utils/download_from_google_drive.sh \\\n",
        "    \"https://drive.google.com/open?id=1AhtRB0vTkjDrum-dfgaiXnQgsAAiYMGW\" \\\n",
        "    downloads/wavenet_vocoder_pretrained\n",
        "\n",
        "# download wav samples\n",
        "!../PytorchWaveNetVocoder/wavenet_vocoder/utils/download_from_google_drive.sh \\\n",
        "    \"https://drive.google.com/open?id=1kBwF7ejyCR5aI9FitmMSCnWdPCNVouqg\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBqqjMhER1U1"
      },
      "source": [
        "- Sprocket pretrained model\n",
        "    - `GMM_mcep.pkl`: GMM param file for mcep conversion.\n",
        "    - `<src_spk>.yml`: Source speaker yaml file.\n",
        "    - `<src_spk>-<tar_spk>.yml`: Source-target speaker pair yaml file.\n",
        "    - `<src_spk>.h5`: Statistics file of source speaker.\n",
        "    - `<tar_spk>.h5`: Statistics file of target speaker.\n",
        "    - `cvgv.h5`: Statistics file of global variance for converted features.\n",
        "    \n",
        "- Target speaker WaveNet vocoder pretrained model\n",
        "    - `model.conf`: Model configuration file.\n",
        "    - `checkpoint-*.pkl`: Model parameter file.\n",
        "    - `stats.h5`: Statistics file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptngq0hLR1U1"
      },
      "outputs": [],
      "source": [
        "!ls downloads/*pretrained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vyxhW4OR1U1"
      },
      "source": [
        "Next, extract features and then convert them to target speaker's one.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5xjMoc0R1U1"
      },
      "outputs": [],
      "source": [
        "![ ! -e hdf5 ] && mkdir hdf5\n",
        "![ ! -e wav ] && mkdir wav\n",
        "!PYTHONPATH=../sprocket/example/src \\\n",
        "    python ../sprocket/sprocket/bin/convert_feats.py \\\n",
        "        --cvmcep0th True \\\n",
        "        --cvcodeap True \\\n",
        "        --cvgvstats downloads/sprocket_pretrained/cvgv.h5 \\\n",
        "        --org_yml downloads/sprocket_pretrained/rms.yml \\\n",
        "        --pair_yml downloads/sprocket_pretrained/rms-slt.yml \\\n",
        "        --org_stats downloads/sprocket_pretrained/rms.h5 \\\n",
        "        --tar_stats downloads/sprocket_pretrained/slt.h5 \\\n",
        "        --mcepgmmf downloads/sprocket_pretrained/GMM_mcep.pkl \\\n",
        "        --iwav downloads/samples/src/arctic_b0536.wav \\\n",
        "        --cvfeats hdf5/arctic_b0536.h5 \\\n",
        "        --owav wav/arctic_b0536.wav\n",
        "!ls hdf5 wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q2q8zWOR1U1"
      },
      "source": [
        "Then generate waveform with pretrained wavenet using converted features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxF-2QrPR1U1"
      },
      "outputs": [],
      "source": [
        "# NOTE: require too much time.\n",
        "# decode with wavenet vocoder\n",
        "!find hdf5 -name \"*.h5\" > hdf5/feats.scp\n",
        "!python ../PytorchWaveNetVocoder/wavenet_vocoder/bin/decode.py \\\n",
        "     --feats hdf5/feats.scp \\\n",
        "     --outdir wav_wnv \\\n",
        "     --checkpoint downloads/wavenet_vocoder_pretrained/checkpoint-final.pkl \\\n",
        "     --fs 16000 \\\n",
        "     --n_gpus 1 \\\n",
        "     --batch_size 4\n",
        "# apply noise shaping filter\n",
        "!find wav_wnv -name \"*.wav\" > wav_wnv/wav.scp\n",
        "!python ../PytorchWaveNetVocoder/wavenet_vocoder/bin/noise_shaping.py \\\n",
        "     --waveforms wav_wnv/wav.scp \\\n",
        "     --outdir wav_wnv_nsf \\\n",
        "     --stats downloads/wavenet_vocoder_pretrained/stats.h5 \\\n",
        "     --fs 16000 \\\n",
        "     --shiftms 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "KCoyeyKfR1U6"
      },
      "outputs": [],
      "source": [
        "# listen to pre-synthesized ones\n",
        "import IPython.display\n",
        "print(\"Source\")\n",
        "IPython.display.display(IPython.display.Audio(\"downloads/samples/src/arctic_b0536.wav\"))\n",
        "print(\"Target\")\n",
        "IPython.display.display(IPython.display.Audio(\"downloads/samples/tar/arctic_b0536.wav\"))\n",
        "print(\"Converted voice with vocoder\")\n",
        "IPython.display.display(IPython.display.Audio(\"downloads/samples/vocoder/arctic_b0536.wav\"))\n",
        "print(\"Converted voice with wavenet vocoder\")\n",
        "IPython.display.display(IPython.display.Audio(\"downloads/samples/wavenet_vocoder/arctic_b0536.wav\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jGwMae_R1U6"
      },
      "outputs": [],
      "source": [
        "print(\"running time = %s minite\" % ((time.time() - start_time) / 60))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z4oVpV_R1U7"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "- Introduced voice conversion with direct waveform modeling\n",
        "- Introduced Sprocket /  PytorchWaveNetVocoder\n",
        "    - Can build GMM-based VC / DIFFVC  & WaveNet vocoder\n",
        "    - Can combine both module to generate high quality converted voices\n",
        "\n",
        "Thank you for your attendance!  \n",
        "If you have time, please send us feedback via [Google form](https://forms.gle/28QrvGRBAAiKpWas8)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Slideshow",
    "colab": {
      "name": "wavenet_vocoder.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "rise": {
      "scroll": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}